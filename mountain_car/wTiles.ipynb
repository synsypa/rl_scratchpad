{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from mountain_car import MountainCar\n",
    "from tilecoding import TileCoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_per_dim = [8, 8]\n",
    "pos_lims = (env.observation_space.low[0], env.observation_space.high[0])\n",
    "spd_lims = (env.observation_space.low[1], env.observation_space.high[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = [pos_lims, spd_lims]\n",
    "n_tilings = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TileCoder(tiles_per_dim, lims, n_tilings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 38, 120, 201, 282])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T[env.reset()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(T.n_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarTiles:\n",
    "    def __init__(self, env, algo, alpha, gamma, epsilon, tiles_per_dim, n_tilings):\n",
    "        self.env = env\n",
    "        self.algo = algo\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.tiles_per_dim = tiles_per_dim\n",
    "        self.n_tilings = n_tilings\n",
    "        lims = [(self.env.observation_space.low[0], self.env.observation_space.high[0]), /\n",
    "                (self.env.observation_space.low[1], self.env.observation_space.high[1])]\n",
    "        self.T = TileCoder(self.tiles_per_dim, lims, self.n_tilings)\n",
    "        self.n_states = self.T.n_tiles\n",
    "        self.Q = np.zeros(self.n_states)\n",
    "        self.ep_reward = []\n",
    "        self.reward_list = []\n",
    "        self.episodes = 0\n",
    "    \n",
    "    def _discretize_state(self, state):\n",
    "        return np.round((state - self.env.observation_space.low)*np.array([20, 200]),0).astype(int)\n",
    "    \n",
    "    def _epsilon_greed(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.env.action_space.n-1)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state[0], state[1]])\n",
    "    \n",
    "    def _get_expected_reward(self, state):\n",
    "        actions = [i for i in range(self.env.action_space.n)]\n",
    "        # Find greedy action, (1-ep)*greedy reward\n",
    "        greedy_action = np.argmax(self.Q[state[0], state[1]])\n",
    "        greedy_reward = (1-self.epsilon) * self.Q[state[0], state[1], greedy_action]\n",
    "        # other actions, ep/n_actions * reward\n",
    "        # actions.remove(greedy_action)\n",
    "        other_rewards = 0\n",
    "        for r in actions:\n",
    "            other_rewards += (self.epsilon/len(actions)) * self.Q[state[0], state[1], r]\n",
    "        return greedy_reward + other_rewards\n",
    "    \n",
    "    def _calc_avg_reward(self):\n",
    "        avg_ep_reward = np.mean(self.ep_reward)\n",
    "        self.reward_list.append(avg_ep_reward)\n",
    "        self.ep_reward = []\n",
    "        return(avg_ep_reward)\n",
    "    \n",
    "    def run_episode(self, decay, render = False):\n",
    "        if self.episodes % 500 == 0 or render:\n",
    "            print(f'Running episode {self.episodes} using {self.algo}, epsilon={round(self.epsilon,5)}, alpha={self.alpha}, discount={self.gamma}')\n",
    "        end = False\n",
    "        total_reward, reward = 0,0\n",
    "        # Initial State\n",
    "        S = self.env.reset()\n",
    "        # Discreize State\n",
    "        S_dis = self._discretize_state(S)\n",
    "        while not end:\n",
    "            if render:\n",
    "                env.render()\n",
    "                \n",
    "            action = self._epsilon_greed(S_dis)\n",
    "            S_next, reward, end, _ = self.env.step(action)\n",
    "            S_dis_next = self._discretize_state(S_next)\n",
    "            \n",
    "            # If end of episode\n",
    "            if end and S_next[0] >= 0.5:\n",
    "                self.Q[S_dis[0], S_dis[1], action] = reward\n",
    "                \n",
    "            # otherwise update according to chosen algorithm\n",
    "            else:\n",
    "                if self.algo == 'q':\n",
    "                    next_reward = np.max(self.Q[S_dis_next[0], S_dis_next[1]])\n",
    "                elif self.algo == 'expected_sarsa':\n",
    "                    next_reward = self._get_expected_reward(S_dis_next)\n",
    "                elif self.algo == 'sarsa':\n",
    "                    next_reward = self.Q[S_dis_next[0], S_dis_next[1], self._epsilon_greed(S_dis_next)]\n",
    "                    \n",
    "                delta = self.alpha * (reward + (self.gamma * next_reward) - self.Q[S_dis[0], S_dis[1], action])\n",
    "                self.Q[S_dis[0], S_dis[1], action] += delta\n",
    "                \n",
    "            S_dis = S_dis_next\n",
    "            total_reward += reward\n",
    "            \n",
    "        if render:\n",
    "            if (S_next[0] >= 0.5):\n",
    "                print('Success :)')\n",
    "            else: \n",
    "                print('Failure :(')\n",
    "            time.sleep(1)\n",
    "            \n",
    "        self.ep_reward.append(total_reward)\n",
    "        self.episodes += 1\n",
    "        self.epsilon -= decay\n",
    "        if self.episodes % 100 == 0:\n",
    "            avg_reward = self._calc_avg_reward()\n",
    "            if self.episodes % 500 == 0:\n",
    "                print(f'Avg Reward Over Last 100 Episodes = {avg_reward}...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
